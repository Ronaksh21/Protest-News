{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "protest_news-summ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4cr/l+zOXEuM93fwdJBvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codesanta/Protest-News/blob/master/protest_news_summ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdEBYlbmuerC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This Jupyter Notebook Has the Code which Takes the Full Articles and Summarises them using PageRank algorithm and Compares the results with the reference to generate rouge scores for evaluations. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwL3w6t9wMJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8249bd68-e082-487c-9c5b-defe687132eb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "# data=pd.read_csv(\"traindata.csv\")\n",
        "# cont=data.content\n",
        "# text=cont[0]\n",
        " \n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    data=pd.read_csv(\"traindata.csv\")\n",
        "    cont=data.content\n",
        "    text=cont[2]\n",
        "    article = text.split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        #print(sentence)\n",
        "        lel = sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \")\n",
        "        # # lel.remove(\"\")\n",
        "        # lel.remove(\" \")\n",
        "        # lel.remove(\"  \")\n",
        "        sentences.append(lel)\n",
        "    sentences.pop() \n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "    #print(sentences)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"file.txt\", 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Summarize Text: \n",
            " Residents of Javagal hobli in Arsikere taluk, here on Wednesday, staged a protest demanding fair compensation for their land being acquired for Yettinahole project, besides water for the villages under the project.\r\n",
            "Hundreds of people from the hobli staged a dharna in front of the Deputy Commissioner's office. “We are demanding the same compensation for our land\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fnATCdow8hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "On 23 December, veteran artistes and poets including members of AASU assembled in Guwahati (Kamrup district, Assam) and held protest against the Citizenship (Amendment) Act, 2019. [size=no report]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcWPKMWU2cg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1ff96498-c40d-4501-eaa4-5bce6dccd5cf"
      },
      "source": [
        "pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgFhZJoS2c_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge \n",
        "\n",
        "hypothesis = \"Residents of Javagal hobli in Arsikere taluk, here on Wednesday, staged a protest demanding fair compensation for their land being acquired for Yettinahole project, besides water for the villages under the project.Hundreds of people from the hobli staged a dharna in front of the Deputy Commissioner's office. “We are demanding the same compensation for our land\"\n",
        "reference = \"On December 4, hundreds of residents staged a protest in front of the Deputy Commissioner's office in Hassan city (Hassan, Karnataka), demanding fair compensation for their land being acquired for Yettinahole project, besides water for the villages under the project.\"\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2QCE0rZ2jM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c88e1e6f-62b4-43c8-ddfc-cd4bdbcc8549"
      },
      "source": [
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'rouge-1': {'f': 0.6391752528855352, 'p': 0.543859649122807, 'r': 0.775}, 'rouge-2': {'f': 0.5473684162127425, 'p': 0.4642857142857143, 'r': 0.6666666666666666}, 'rouge-l': {'f': 0.6666666617102223, 'p': 0.6097560975609756, 'r': 0.7352941176470589}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ynXgPf2k0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}